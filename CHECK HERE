# Example collate_batch function
def collate_batch(batch):
    label_list, text_list, lengths = [], [], []
    for _label, _text in batch:
        tokenized_text = [vocab[token] for token in tokenizer(_text)]
        if len(tokenized_text) > 0:  # Ensure non-empty sequences
            label_list.append(label_pipeline(_label))
            processed_text = torch.tensor(tokenized_text, dtype=torch.int64)
            text_list.append(processed_text)
            lengths.append(processed_text.size(0))
    if len(text_list) == 0:
        return torch.tensor([]), torch.tensor([]), torch.tensor([])
    label_list = torch.tensor(label_list)
    lengths = torch.tensor(lengths)
    padded_text_list = nn.utils.rnn.pad_sequence(text_list, batch_first=True)
    return padded_text_list, label_list, lengths
